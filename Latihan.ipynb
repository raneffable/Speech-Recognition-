{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Asus\\AppData\\Local\\Temp/ipykernel_17052/3597539986.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isdir, join\n",
    "import librosa\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import python_speech_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bom Kanan\n",
      "Bom Kiri\n",
      "Hidup\n",
      "Jalan Kanan\n",
      "Jalan Kiri\n",
      "Lompat\n",
      "Lompat Kanan\n",
      "Lompat Kiri\n",
      "Nunduk\n",
      "Nunduk Tembak\n",
      "Tembak\n",
      "Tembak Atas\n",
      "Tembak Kiri\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'data\\mini_speech_commands'\n",
    "for name in listdir(dataset_path):\n",
    "    if isdir(join(dataset_path, name)):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bom Kanan', 'Bom Kiri', 'Hidup', 'Jalan Kanan', 'Jalan Kiri', 'Lompat', 'Lompat Kanan', 'Lompat Kiri', 'Nunduk', 'Nunduk Tembak', 'Tembak', 'Tembak Atas', 'Tembak Kiri']\n"
     ]
    }
   ],
   "source": [
    "all_target = [name for name in listdir(dataset_path) if isdir(join(dataset_path, name))]\n",
    "print(all_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "Total sample: 39\n"
     ]
    }
   ],
   "source": [
    "#To see how many files are in each\n",
    "num_samples = 0\n",
    "for target in all_target:\n",
    "    print(len(listdir(join(dataset_path,name))));\n",
    "    num_samples += len(listdir(join(dataset_path,name)))\n",
    "print(f\"Total sample: {num_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "target_list = all_target\n",
    "feature_sets_file = 'all_target_mfcc_sets.npz'\n",
    "perc_keep_samples = 1.0 # 1.0 keep all the samples\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "sample_rate = 16000\n",
    "num_mfcc = 16\n",
    "len_mfcc = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\mini_speech_commands\\Bom Kanan\n",
      "data\\mini_speech_commands\\Bom Kiri\n",
      "data\\mini_speech_commands\\Hidup\n",
      "data\\mini_speech_commands\\Jalan Kanan\n",
      "data\\mini_speech_commands\\Jalan Kiri\n",
      "data\\mini_speech_commands\\Lompat\n",
      "data\\mini_speech_commands\\Lompat Kanan\n",
      "data\\mini_speech_commands\\Lompat Kiri\n",
      "data\\mini_speech_commands\\Nunduk\n",
      "data\\mini_speech_commands\\Nunduk Tembak\n",
      "data\\mini_speech_commands\\Tembak\n",
      "data\\mini_speech_commands\\Tembak Atas\n",
      "data\\mini_speech_commands\\Tembak Kiri\n"
     ]
    }
   ],
   "source": [
    "filenames = []\n",
    "y = []\n",
    "for index, target in enumerate(target_list):\n",
    "    print(join(dataset_path,target))\n",
    "    filenames.append(listdir(join(dataset_path,target)))\n",
    "    y.append(np.ones(len(filenames[index])) * index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 0., 0.]), array([1., 1., 1.]), array([2., 2., 2.]), array([3., 3., 3.]), array([4., 4., 4.]), array([5., 5., 5.]), array([6., 6., 6.]), array([7., 7., 7.]), array([8., 8., 8.]), array([9., 9., 9.]), array([10., 10., 10.]), array([11., 11., 11.]), array([12., 12., 12.])]\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "for item in y:\n",
    "    print(len(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_y = list(zip(filenames, y))\n",
    "random.shuffle(filenames_y)\n",
    "filenames, y = zip(*filenames_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(filenames))\n",
    "filenames = filenames[:int(len(filenames)* perc_keep_samples)]\n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_set_size = int(len(filenames) * val_ratio)\n",
    "test_set_size = int(len(filenames) * test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_val = filenames[:val_set_size]\n",
    "filenames_test = filenames[val_set_size:(val_set_size + test_set_size)]\n",
    "filenames_train = filenames[(val_set_size + test_set_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_orig_val = y[:val_set_size]\n",
    "y_orig_test = y[val_set_size:(val_set_size + test_set_size)]\n",
    "y_orig_train = y[(val_set_size + test_set_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mfcc(path):\n",
    "    #Load wavefiles\n",
    "    signal , fs = librosa.load(path, sr=sample_rate)\n",
    "\n",
    "    #Create MFCCs from soundclip\n",
    "    mfccs = python_speech_features.base.mfcc(signal,\n",
    "    samplerate=fs,winlen = 0.256, winstep = 0.050, numcep = num_mfcc, nfilt = 26,\n",
    "    nfft = 2048, preemph = 0.0, ceplifter = 0, appendEnergy = False, winfunc = np.hanning)\n",
    "\n",
    "    return mfcc.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17052/235997324.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Create path from given filename and target item\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     path = join(dataset_path, target_list[int(y_orig_train[index])], \n\u001b[0m\u001b[0;32m     13\u001b[0m                 filename)\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "# TEST: Construct test set by computing MFCC of each WAV file\n",
    "prob_cnt = 0\n",
    "x_test = []\n",
    "y_test = []\n",
    "for index, filename in enumerate(filenames_train):\n",
    "    \n",
    "    # Stop after 500\n",
    "    if index >= 500:\n",
    "        break\n",
    "    \n",
    "    # Create path from given filename and target item\n",
    "    path = join(dataset_path, target_list[int(y_orig_train[index])], \n",
    "                filename)\n",
    "    \n",
    "    # Create MFCCs\n",
    "    mfccs = calc_mfcc(path)\n",
    "    \n",
    "    if mfccs.shape[1] == len_mfcc:\n",
    "        x_test.append(mfccs)\n",
    "        y_test.append(y_orig_train[index])\n",
    "    else:\n",
    "        print('Dropped:', index, mfccs.shape)\n",
    "        prob_cnt += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "577c0e9348674e6ca19739c53e1dcb2a2fa45990424a1cb88fce5cdd2dd0a713"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
